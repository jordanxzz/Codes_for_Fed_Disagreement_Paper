# -*- coding: utf-8 -*-
"""Transcripts_Measure.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/192hkvGcMEf7N5RvHoBhrQVXUxrD33j_d

Preparation and Installation
"""

!pip install glove_python
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('popular')

"""Importing modules"""

import os
from collections import Counter, defaultdict, OrderedDict
import re
from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer
from nltk import pos_tag
import pandas as pd
import numpy as np
import scipy
import nltk
import pickle
#import glove.Corpus as Corpus
#import glove.Glove as Glove
from glove import Corpus, Glove
regword_tokenizer = RegexpTokenizer(r'\w+')
lemmatizer = nltk.stem.WordNetLemmatizer()
stop_words = set(nltk.corpus.stopwords.words('english'))

"""The functions"""

#####The Naive method#####
def get_transcripts_by_meeting(df_labelled_transcripts):
    df_transcripts_no_front = df_labelled_transcripts[df_labelled_transcripts['Speaker'] != 'FRONT_MATTER']
    df_transcripts_by_meeting = df_transcripts_no_front.groupby('Meeting_Date')['Content'].apply(list).reset_index()
    return df_transcripts_by_meeting

def count_interrogative(transcript_text):
    interrogative_list = ['what', 'where', 'when', 'who', 'how', 'why', 'which', 'whose', '?']
    text_length = len(transcript_text)
    tokens = [token for token in word_tokenize(str(transcript_text))]
    raw_count = len([token for token in tokens if token in interrogative_list])
    return raw_count, text_length
    
def get_naive_measure(transcripts_list):
    naive_interrogative_count_raw, naive_interrogative_count_normalized = [], []
    for transcript_text in transcripts_list:
        raw_count, text_length = count_interrogative(transcript_text)
        naive_interrogative_count_raw.append(raw_count)
        naive_interrogative_count_normalized.append(raw_count/text_length*10000)
    return naive_interrogative_count_raw, naive_interrogative_count_normalized

#####The GloVe method#####
def get_wn_pos(ptb_pos):
    noun = ['NN', 'NNS']
    verb = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']
    adj = ['JJ', 'JJR', 'JJS']
    adv = ['RB', 'RBR', 'RBS']
    if ptb_pos in noun:
        return 'Noun'
    elif ptb_pos in verb:
        return 'Verb'
    elif ptb_pos in adj:
        return 'Adj'
    elif ptb_pos in adv:
        return 'Adv'
    else:
        return ptb_pos

def get_seed_words(positive, negative):
    positive_seed_words = [(lemmatizer.lemmatize(tuple(x)[0].lower()), tuple(x)[1]) for x in pd.read_csv(positive, encoding='utf-8')[['Lemma', 'PoS']].values]
    negative_seed_words = [(lemmatizer.lemmatize(tuple(x)[0].lower()), tuple(x)[1]) for x in pd.read_csv(negative, encoding='utf-8')[['Lemma', 'PoS']].values]
    return positive_seed_words, negative_seed_words
    
def get_raw_corpus_array(df_transcripts_no_front): # tag added
    raw_corpus_array = [[(lemmatizer.lemmatize(str(tuples[0])).lower(), get_wn_pos(tuples[1])) for tuples in nltk.pos_tag(regword_tokenizer.tokenize(str(document))) if tuples[0] not in stop_words] for document in df_transcripts_no_front.Content.tolist()]
#    raw_corpus_array = [[token for token in regword_tokenizer.tokenize(str(document)) if token not in stop_words] for document in df_transcripts_no_front.Content.tolist()]
    return raw_corpus_array

def get_glove_vectors(raw_corpus_array, window, dim, lrate, epochs):
#    stop_words = set(nltk.corpus.stopwords.words('english'))
#    raw_corpus_array = [[(lemmatizer.lemmatize(tuples[0]), get_wn_pos(tuples[1])) for tuples in raw_tags] for document in df_transcripts_no_front.Content.tolist()]
    # Create a corpus object
    corpus = Corpus()
    # Get the co-occurrence matrix
    corpus.fit (raw_corpus_array, window=window) #window is the context window size
    # Get the glove object to use the matrix
    glove = Glove(no_components=dim, learning_rate=lrate)
    glove.fit(corpus.matrix, epochs=epochs, no_threads=8, verbose=True)
    glove.add_dictionary (corpus.dictionary)
    glove.save('FOMC_glove_'+str(dim)+'_dim.model')
    #glove.word_vectors
    return glove

def get_word_vector(word, glove):
    return np.array(glove.word_vectors[glove.dictionary[word]])

def get_cosine_diff(u,v):
    return 1 - scipy.spatial.distance.cosine(u,v)

def get_word_score(word, positive_seed_words, negative_seed_words, glove, vocabulary):
    score = 0
    word_vector = get_word_vector(word, glove)
    for positive_seed in positive_seed_words:
        if positive_seed in vocabulary:
            positive_vector = get_word_vector(positive_seed, glove)
            score += get_cosine_diff(word_vector, positive_vector)
    for negative_seed in negative_seed_words:
        if negative_seed in vocabulary:
            negative_vector = get_word_vector(negative_seed, glove)
            score -= get_cosine_diff(word_vector, negative_vector)
    return score
### maybe need to normalize the scores

### vocabulary of lemma-PoS pair
def get_vocabulary(raw_corpus_array):
    pos_filter = ['NNP', 'NNPS', 'CD', 'EX', 'FW', 'SYM', 'TO']
    return set([word for text in raw_corpus_array for word in text if word[1] not in pos_filter]) # need filter word type

def get_ordered_vocab(vocabulary, positive_seed_words, negative_seed_words, glove): # need to update the normalization method!!!!!
    vocabulary_score_dict = {word:get_word_score(word, positive_seed_words, negative_seed_words, glove, vocabulary) for word in vocabulary}
    ordered_vocabulary_list = list(OrderedDict(sorted(vocabulary_score_dict.items(), key=lambda t: t[1])).items())
    min_s, max_s = ordered_vocabulary_list[0][1], ordered_vocabulary_list[-1][1]
    vocabulary_score_dict_n = {word:(score-min_s)/(max_s-min_s)*2-1 for word, score in vocabulary_score_dict.items()}
    ordered_vocabulary_list_n = list(OrderedDict(sorted(vocabulary_score_dict_n.items(), key=lambda t: t[1])).items())    
    return ordered_vocabulary_list, ordered_vocabulary_list_n

def get_lexison(nwords, ordered_vocabulary_list): # positive_lexicon, negative_lexicon
    return dict(ordered_vocabulary_list[-nwords:]), dict(ordered_vocabulary_list[:nwords])

def get_negative_position(tokens):
    negative_signals = ['not', 'no', 'never', 'neither', 'nor']
    for i in range(len(tokens)-1):
        if tokens[i] in negative_signals and (tokens[i], tokens[i+1]) != ('not', 'only'):
            return i
    else:
        return len(tokens)
    
def get_sentiment_score(transcripts_list, positive_lexicon, negative_lexicon):
    lexicon = positive_lexicon.copy()
    lexicon.update(negative_lexicon)
    score_list = []
    total_scores = []
    total_counts = []
    for transcript_text in transcripts_list:
        total_count, total_score = 0, 0
        sentences = sent_tokenize(str(transcript_text))
        for sentence in sentences:
            tokens = word_tokenize(sentence)
            negative_position = get_negative_position(tokens)
            lemma_pairs = [(lemmatizer.lemmatize(tuples[0]).lower(), get_wn_pos(tuples[1])) for tuples in nltk.pos_tag(tokens)]
            for i in range(len(lemma_pairs)):
                if i > negative_position:
                    negation = -1
                else:
                    negation = 1
                if lemma_pairs[i] in lexicon:
                    total_count += 1
                    total_score += lexicon[lemma_pairs[i]] * negation
        try:
            score_list.append(total_score/total_count)
        except:
            score_list.append('')
        total_scores.append(total_score)
        total_counts.append(total_count)
#         print(total_scores)
    return score_list, total_scores, total_counts

if __name__ == "__main__":
##########Files needed: positive and negative seed words, FOMC_transcripts_df.pickle
################################################################################
##########Preparation
#    data_dir = "C:/Users/xiazizhe.HKUPC2.006/Downloads"
    file_name = "FOMC_Transcripts.txt"
    positive, negative = "Positive_Seed_Words.csv", "Negative_Seed_Words.csv"
    ###Read file and get raw data###
#    os.chdir(data_dir)
    positive_seed_words, negative_seed_words = get_seed_words(positive, negative)
#    df_labelled_transcripts = pd.read_csv(file_name, encoding='utf-8', sep="|", engine='c', error_bad_lines=False)
    df_labelled_transcripts = pickle.load(open("FOMC_transcripts_df_1993.pickle", "rb"))
    df_transcripts_no_front = df_labelled_transcripts[df_labelled_transcripts['Speaker'] != 'FRONT_MATTER']
    df_transcripts_by_meeting = df_transcripts_no_front.groupby('Meeting_Date')['Content'].apply(list).reset_index()
    meeting_date_list = df_transcripts_by_meeting.Meeting_Date.tolist()
    transcripts_list = df_transcripts_by_meeting.Content.tolist()
    ###Naive Measure for cross checking###
    naive_interrogative_count_raw, naive_interrogative_count_normalized = get_naive_measure(transcripts_list)
    ###Convert to corpus array###
    raw_corpus_array = get_raw_corpus_array(df_transcripts_no_front)
    ###Get vocabulary###
    vocabulary = get_vocabulary(raw_corpus_array)
    pickle.dump(vocabulary, open("vocabulary.pickle", "wb"))
    for word in positive_seed_words + negative_seed_words:
        if word not in vocabulary:
            print(word)
################################################################################
    ###GloVe vector representation 100 dim###
    results = {}
    dims = [50, 100, 150, 200]
    for dim in dims:
        glove = get_glove_vectors (raw_corpus_array, 15, dim, 0.01, 2)
        ###Get ordered vocabulary###
        ordered_vocabulary_list, ordered_vocabulary_list_n = get_ordered_vocab(vocabulary, positive_seed_words, negative_seed_words, glove)
        pickle.dump(ordered_vocabulary_list, open(str(dim)+"dim_ordered_vocabulary.pickle", "wb"))
        ###Get lexicon###
        nwords_list = [100, 250, 500, 750, 1000, 1500] # nwords per lexicon (positive and negative)
        for nwords in nwords_list:
            positive_lexicon, negative_lexicon = get_lexison(nwords, ordered_vocabulary_list)
            prefix = str(dim)+"_dim_"+str(nwords)+"_words_"
            results[prefix+"senti_score"], results[prefix+"total_score"], results[prefix+"total_count"] = get_sentiment_score(transcripts_list, positive_lexicon, negative_lexicon)
    df = pd.DataFrame(results)
    df.to_csv("FOMC_transcripts_measure.csv", index=False)
    pickle.dump((positive_lexicon, negative_lexicon), open("lexicons.pickle", "wb"))
#     print(positive_lexicon)
#     print(negative_lexicon)
#     print(positive_lexicon_n)
#     print(negative_lexicon_n)